{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc.stop()\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header ='true',inferschema ='true').load('D:/jcu/5851/A4/unit_2020.csv')\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unit description: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.select(['unit description','Category'])\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Category|count|\n",
      "+--------+-----+\n",
      "|    EDST|   68|\n",
      "|    MMBA|   65|\n",
      "|    LAWS|   48|\n",
      "|    ACCG|   44|\n",
      "|    COMP|   43|\n",
      "|    TRAN|   41|\n",
      "|    AFCP|   41|\n",
      "|    STAT|   39|\n",
      "|    MGMT|   36|\n",
      "|    PICT|   36|\n",
      "|    SPED|   32|\n",
      "|    AFIN|   32|\n",
      "|    PICX|   32|\n",
      "|    MEDI|   29|\n",
      "|    MKTG|   28|\n",
      "|    PSYN|   27|\n",
      "|    AFCL|   27|\n",
      "|    MMCC|   25|\n",
      "|    PHTY|   25|\n",
      "|    GMBA|   24|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 20 group label and count\n",
    "from pyspark.sql.functions import col\n",
    "data.groupBy('Category').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP method 1 : word count vectors\n",
    "# ML method 1 : Logistic Regression\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "# inputCol: description\n",
    "# outputCol: stop words removed\n",
    "regexTokenizer2 = RegexTokenizer(inputCol='unit description', outputCol='words', pattern='\\\\W')\n",
    "# stop words\n",
    "add_stopwords = ['unit','study','course','studies','field','students', 'faculty','staff','be','work','form','this']\n",
    "stopwords_remover2 = StopWordsRemover(inputCol='words', outputCol='filtered').setStopWords(add_stopwords)\n",
    "# words vector\n",
    "count_vectors2 = CountVectorizer(inputCol='filtered', outputCol='features', vocabSize=10000, minDF=5)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol='Category', outputCol='label')\n",
    "pipeline = Pipeline(stages=[regexTokenizer2, stopwords_remover2, count_vectors2, label_stringIdx])\n",
    "# fit the pipeline to documents\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipeline_fit.transform(data)\n",
    "#dataset.filter(dataset['Category'] == 'ACST').select(['filtered','features','Category','label']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count:829\n",
      "Test Dataset Count:371\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "# training / test setting，7:3\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print('Training Dataset Count:{}'.format(trainingData.count()))\n",
    "print('Test Dataset Count:{}'.format(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4762085226558802\n",
      "Running time: 3.90097713470459\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(maxIter=50, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').orderBy('probability', accending=False).show(n=10, truncate=30)\n",
    "\n",
    "# predictionCol: prediction column\n",
    "evaluator2 = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "# accuracy\n",
    "print(\"Accuracy: \" + str(evaluator2.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3633177066932591\n",
      "Running time: 1.7193336486816406\n"
     ]
    }
   ],
   "source": [
    "# NLP method 1 : word count vectors\n",
    "# ML method 2 : Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "#predictions.filter(predictions['prediction'] == 16) \\\n",
    "#     .select( 'Category', 'probability', 'label', 'prediction') \\\n",
    "#     .orderBy('probability', ascending=False) \\\n",
    "#     .show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36603501384644005\n",
      "Running time: 9.490335464477539\n"
     ]
    }
   ],
   "source": [
    "# NLP method 1 : word count vectors\n",
    "# ML method 3 : Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol='label', \\\n",
    "                             featuresCol='features', \\\n",
    "                             numTrees=100, \\\n",
    "                             maxDepth=10, \\\n",
    "                             maxBins=64)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "#predictions.filter(predictions['prediction'] == 16) \\\n",
    "#     .select('Category','probability','label','prediction') \\\n",
    "#     .orderBy('probability', ascending=False) \\\n",
    "#     .show(n = 10, truncate = 30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4568939323939996\n",
      "Running time: 8.052201986312866\n"
     ]
    }
   ],
   "source": [
    "# NLP method 2 : TF-IDF\n",
    "# ML method 1 : Logistic Regression\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "start_time = time.time()\n",
    "hashingTF = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features', minDocFreq=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer2, stopwords_remover2, hashingTF, idf, label_stringIdx])\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipeline_fit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    " \n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lr_model = lr.fit(trainingData)\n",
    "predictions = lr_model.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44377322242332906\n",
      "Running time: 3.4365761280059814\n"
     ]
    }
   ],
   "source": [
    "# NLP method 2 : TF-IDF\n",
    "# ML method 2 : Naive Bayes\n",
    "#from pyspark.ml.classification import NaiveBayes\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "#predictions.filter(predictions['prediction'] == 16) \\\n",
    "#     .select( 'Category', 'probability', 'label', 'prediction') \\\n",
    "#     .orderBy('probability', ascending=False) \\\n",
    "#     .show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3558465402119936\n",
      "Running time: 11.208366632461548\n"
     ]
    }
   ],
   "source": [
    "# NLP method 2 : TF-IDF\n",
    "# ML method 3 : Random Forest\n",
    "#from pyspark.ml.classification import RandomForestClassifier\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol='label', \\\n",
    "                             featuresCol='features', \\\n",
    "                             numTrees=100, \\\n",
    "                             maxDepth=10, \\\n",
    "                             maxBins=64)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "#predictions.filter(predictions['prediction'] == 16) \\\n",
    "#     .select('Category','probability','label','prediction') \\\n",
    "#     .orderBy('probability', ascending=False) \\\n",
    "#     .show(n = 10, truncate = 30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP method 3 : tagging + TF-IDF\n",
    "# ML method 1 : Logistic Regression\n",
    "import nltk\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "#from nltk.stem.porter import *\n",
    "\n",
    "tags = set(['NN','NNS','NNP','NNPS','JJ','VB','VBG','VBN'])\n",
    "\n",
    "def pos_tag(text):\n",
    "    text2 = nltk.word_tokenize(text.lower())\n",
    "    pos_tags = nltk.pos_tag(text2)\n",
    "    ret = []\n",
    "    for word,pos in pos_tags:\n",
    "        if (pos in tags and word not in add_stopwords):\n",
    "            ret.append(word)\n",
    "    ret= sorted(set(ret))\n",
    "    return ret\n",
    "udfValueToList = udf(pos_tag, ArrayType(StringType()))\n",
    "data = data.withColumn('filtered2', udfValueToList('unit description'))\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol='filtered2', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features', minDocFreq=5)\n",
    "pipeline = Pipeline(stages=[hashingTF,idf, label_stringIdx])\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipeline_fit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4462921180450076\n",
      "Running time: 217.89178943634033\n"
     ]
    }
   ],
   "source": [
    "# NLP method 3 : tagging + TF-IDF\n",
    "# ML method 1 : Logistic Regression\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(maxIter=30, regParam=0.3, elasticNetParam=0)\n",
    "lr_model = lr.fit(trainingData)\n",
    "predictions = lr_model.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42009022259059275\n",
      "Running time: 195.4695110321045\n"
     ]
    }
   ],
   "source": [
    "# NLP method 3 : tagging + TF-IDF\n",
    "# ML method 2 : Naive Bayes\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3451141350948708\n",
      "Running time: 343.14731669425964\n"
     ]
    }
   ],
   "source": [
    "# NLP method 3 : tagging + TF-IDF\n",
    "# ML method 3 : Random Forest\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol='label', \\\n",
    "                             featuresCol='features', \\\n",
    "                             numTrees=100, \\\n",
    "                             maxDepth=10, \\\n",
    "                             maxBins=64)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP method 4 : word2vec\n",
    "# ML method 1 : Logistic Regression\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "# inputCol: description\n",
    "# outputCol: stop words removed\n",
    "#regexTokenizer = RegexTokenizer(inputCol='unit description', outputCol='words', pattern='\\\\W')\n",
    "# stop words\n",
    "#add_stopwords = ['unit','study','course','studies','field','students', 'faculty','staff','be','work','form','this']\n",
    "#stopwords_remover = StopWordsRemover(inputCol='words', outputCol='filtered2').setStopWords(add_stopwords)\n",
    "word2vec = Word2Vec(inputCol=\"filtered\", outputCol=\"features\")\n",
    "#label_stringIdx = StringIndexer(inputCol='Category', outputCol='label')\n",
    "pipeline = Pipeline(stages=[regexTokenizer2,stopwords_remover2,word2vec, label_stringIdx])\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipeline_fit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11781491525923361\n",
      "Running time: 5.182934045791626\n"
     ]
    }
   ],
   "source": [
    "# NLP method 4 : word2vec\n",
    "# ML method 1 : Logistic Regression\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)\n",
    "lr_model = lr.fit(trainingData)\n",
    "predictions = lr_model.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8910.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 296.0 failed 1 times, most recent failure: Lost task 0.0 in stage 296.0 (TID 494, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0494010655498552,0.021606753440239834,0.03914478185899617,-0.010815896261669506,-0.013193660172381912,0.008746300763870422,-0.019435288443241543,-0.05210639878470054,-0.01974282363159143,0.006487035205431252,0.01258337291574953,-0.01793211557116115,0.02767387569546089,0.028060258805110563,0.0021220994130402926,-0.018086374219655837,0.015663163044888788,5.990019655923863E-5,-0.015545790104699307,0.010385167792622673,-0.01011081404108974,-0.005657448878007956,0.0356521494782026,-0.00463165836117696,0.0012773031485266984,0.018086207218536893,0.027082354590854014,-0.004533920131268774,0.02622583066256427,0.035140570712354054,-0.025626410573928955,-0.014307745552972937,0.0409695204828301,0.05059781316934978,0.03229498086768187,0.011430295413268395,0.043372122706849224,0.008638833594111512,-0.0068286227989850234,-0.03399408387295047,-0.010867463607058602,-0.010933131236343485,-0.005080345591299282,0.00528033949114138,-0.021758115425282235,0.023224034459238537,-0.027428383927040383,3.1452208994048054E-4,-0.03956714216700839,0.02657825886710837,0.022845761801810846,0.020180753922685372,-0.026208036883112015,-0.02289906810476735,-0.005707501296381481,-0.045015290526474724,-0.0876712493205321,0.039622954285673063,0.03158680147197495,0.011706670907471085,0.06400539029340001,-0.011364914955891914,0.024470934901981937,0.0059897492956522795,0.0017948999812971557,0.023944155881314953,-5.355746939503512E-4,-0.0043604756489808326,0.04518240590514921,8.217359747027704E-4,-0.020594808064996587,-0.09719260036754498,-0.028972812428544218,0.025573145878237873,-0.015295128809902666,-0.0035200267476716736,0.007756556983756619,-0.017497207413114547,-0.048209637827163594,0.002353464163549733,-0.06571633131868305,0.0663415342798365,-0.025845657593234764,0.019452739978346546,0.03535028442417913,0.029693517266184886,0.015198410638478264,-0.020510618319185298,-0.03502172753634695,-0.010170216632045074,0.0015087818352841453,-0.050863066673866614,-0.030792137170897523,-0.0013738260842997153,-0.017560946552815742,-0.0029132748013133277,0.033888756848712356,0.028960619080758303,0.03062076564424015,0.0026270865444113028].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0494010655498552,0.021606753440239834,0.03914478185899617,-0.010815896261669506,-0.013193660172381912,0.008746300763870422,-0.019435288443241543,-0.05210639878470054,-0.01974282363159143,0.006487035205431252,0.01258337291574953,-0.01793211557116115,0.02767387569546089,0.028060258805110563,0.0021220994130402926,-0.018086374219655837,0.015663163044888788,5.990019655923863E-5,-0.015545790104699307,0.010385167792622673,-0.01011081404108974,-0.005657448878007956,0.0356521494782026,-0.00463165836117696,0.0012773031485266984,0.018086207218536893,0.027082354590854014,-0.004533920131268774,0.02622583066256427,0.035140570712354054,-0.025626410573928955,-0.014307745552972937,0.0409695204828301,0.05059781316934978,0.03229498086768187,0.011430295413268395,0.043372122706849224,0.008638833594111512,-0.0068286227989850234,-0.03399408387295047,-0.010867463607058602,-0.010933131236343485,-0.005080345591299282,0.00528033949114138,-0.021758115425282235,0.023224034459238537,-0.027428383927040383,3.1452208994048054E-4,-0.03956714216700839,0.02657825886710837,0.022845761801810846,0.020180753922685372,-0.026208036883112015,-0.02289906810476735,-0.005707501296381481,-0.045015290526474724,-0.0876712493205321,0.039622954285673063,0.03158680147197495,0.011706670907471085,0.06400539029340001,-0.011364914955891914,0.024470934901981937,0.0059897492956522795,0.0017948999812971557,0.023944155881314953,-5.355746939503512E-4,-0.0043604756489808326,0.04518240590514921,8.217359747027704E-4,-0.020594808064996587,-0.09719260036754498,-0.028972812428544218,0.025573145878237873,-0.015295128809902666,-0.0035200267476716736,0.007756556983756619,-0.017497207413114547,-0.048209637827163594,0.002353464163549733,-0.06571633131868305,0.0663415342798365,-0.025845657593234764,0.019452739978346546,0.03535028442417913,0.029693517266184886,0.015198410638478264,-0.020510618319185298,-0.03502172753634695,-0.010170216632045074,0.0015087818352841453,-0.050863066673866614,-0.030792137170897523,-0.0013738260842997153,-0.017560946552815742,-0.0029132748013133277,0.033888756848712356,0.028960619080758303,0.03062076564424015,0.0026270865444113028].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-244d672a0120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8910.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 296.0 failed 1 times, most recent failure: Lost task 0.0 in stage 296.0 (TID 494, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0494010655498552,0.021606753440239834,0.03914478185899617,-0.010815896261669506,-0.013193660172381912,0.008746300763870422,-0.019435288443241543,-0.05210639878470054,-0.01974282363159143,0.006487035205431252,0.01258337291574953,-0.01793211557116115,0.02767387569546089,0.028060258805110563,0.0021220994130402926,-0.018086374219655837,0.015663163044888788,5.990019655923863E-5,-0.015545790104699307,0.010385167792622673,-0.01011081404108974,-0.005657448878007956,0.0356521494782026,-0.00463165836117696,0.0012773031485266984,0.018086207218536893,0.027082354590854014,-0.004533920131268774,0.02622583066256427,0.035140570712354054,-0.025626410573928955,-0.014307745552972937,0.0409695204828301,0.05059781316934978,0.03229498086768187,0.011430295413268395,0.043372122706849224,0.008638833594111512,-0.0068286227989850234,-0.03399408387295047,-0.010867463607058602,-0.010933131236343485,-0.005080345591299282,0.00528033949114138,-0.021758115425282235,0.023224034459238537,-0.027428383927040383,3.1452208994048054E-4,-0.03956714216700839,0.02657825886710837,0.022845761801810846,0.020180753922685372,-0.026208036883112015,-0.02289906810476735,-0.005707501296381481,-0.045015290526474724,-0.0876712493205321,0.039622954285673063,0.03158680147197495,0.011706670907471085,0.06400539029340001,-0.011364914955891914,0.024470934901981937,0.0059897492956522795,0.0017948999812971557,0.023944155881314953,-5.355746939503512E-4,-0.0043604756489808326,0.04518240590514921,8.217359747027704E-4,-0.020594808064996587,-0.09719260036754498,-0.028972812428544218,0.025573145878237873,-0.015295128809902666,-0.0035200267476716736,0.007756556983756619,-0.017497207413114547,-0.048209637827163594,0.002353464163549733,-0.06571633131868305,0.0663415342798365,-0.025845657593234764,0.019452739978346546,0.03535028442417913,0.029693517266184886,0.015198410638478264,-0.020510618319185298,-0.03502172753634695,-0.010170216632045074,0.0015087818352841453,-0.050863066673866614,-0.030792137170897523,-0.0013738260842997153,-0.017560946552815742,-0.0029132748013133277,0.033888756848712356,0.028960619080758303,0.03062076564424015,0.0026270865444113028].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.0494010655498552,0.021606753440239834,0.03914478185899617,-0.010815896261669506,-0.013193660172381912,0.008746300763870422,-0.019435288443241543,-0.05210639878470054,-0.01974282363159143,0.006487035205431252,0.01258337291574953,-0.01793211557116115,0.02767387569546089,0.028060258805110563,0.0021220994130402926,-0.018086374219655837,0.015663163044888788,5.990019655923863E-5,-0.015545790104699307,0.010385167792622673,-0.01011081404108974,-0.005657448878007956,0.0356521494782026,-0.00463165836117696,0.0012773031485266984,0.018086207218536893,0.027082354590854014,-0.004533920131268774,0.02622583066256427,0.035140570712354054,-0.025626410573928955,-0.014307745552972937,0.0409695204828301,0.05059781316934978,0.03229498086768187,0.011430295413268395,0.043372122706849224,0.008638833594111512,-0.0068286227989850234,-0.03399408387295047,-0.010867463607058602,-0.010933131236343485,-0.005080345591299282,0.00528033949114138,-0.021758115425282235,0.023224034459238537,-0.027428383927040383,3.1452208994048054E-4,-0.03956714216700839,0.02657825886710837,0.022845761801810846,0.020180753922685372,-0.026208036883112015,-0.02289906810476735,-0.005707501296381481,-0.045015290526474724,-0.0876712493205321,0.039622954285673063,0.03158680147197495,0.011706670907471085,0.06400539029340001,-0.011364914955891914,0.024470934901981937,0.0059897492956522795,0.0017948999812971557,0.023944155881314953,-5.355746939503512E-4,-0.0043604756489808326,0.04518240590514921,8.217359747027704E-4,-0.020594808064996587,-0.09719260036754498,-0.028972812428544218,0.025573145878237873,-0.015295128809902666,-0.0035200267476716736,0.007756556983756619,-0.017497207413114547,-0.048209637827163594,0.002353464163549733,-0.06571633131868305,0.0663415342798365,-0.025845657593234764,0.019452739978346546,0.03535028442417913,0.029693517266184886,0.015198410638478264,-0.020510618319185298,-0.03502172753634695,-0.010170216632045074,0.0015087818352841453,-0.050863066673866614,-0.030792137170897523,-0.0013738260842997153,-0.017560946552815742,-0.0029132748013133277,0.033888756848712356,0.028960619080758303,0.03062076564424015,0.0026270865444113028].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# NLP method 4 : word2vec\n",
    "# ML method 2 : Naive Bayes\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23010140579723676\n",
      "Running time: 60.165497064590454\n"
     ]
    }
   ],
   "source": [
    "# NLP method 4 : word2vec\n",
    "# ML method 3 : Random Forest\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol='label', \\\n",
    "                             featuresCol='features', \\\n",
    "                             numTrees=100, \\\n",
    "                             maxDepth=10, \\\n",
    "                             maxBins=64)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "#predictions.filter(predictions['Category'] == 'EDST').select('Category','prediction').\\\n",
    "#orderBy('probability', ascending=False).show(n=10, truncate=30)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48100762430372046\n",
      "Running time: 1679.3602044582367\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "#from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#data = sqlContext.read.format('com.databricks.spark.csv').options(header ='true',inferschema ='true').load('D:/jcu/5851/A4/unit_2020.csv')\n",
    "#data = data.select(['unit description','Category'])\n",
    "start_time = time.time()\n",
    "#regexTokenizer = RegexTokenizer(inputCol='unit description', outputCol='words2', pattern='\\\\W')\n",
    "# stop words\n",
    "#add_stopwords = ['unit','study','course','studies','field','students', 'faculty','staff','be','work','form','this']\n",
    "#stopwords_remover = StopWordsRemover(inputCol='words2', outputCol='filtered3').setStopWords(add_stopwords)\n",
    "# words vector\n",
    "#count_vectors = CountVectorizer(inputCol='filtered3', outputCol='features', vocabSize=10000, minDF=5)\n",
    "#label_stringIdx = StringIndexer(inputCol='Category', outputCol='label')\n",
    "pipeline = Pipeline(stages=[regexTokenizer2, stopwords_remover2, count_vectors2, label_stringIdx])\n",
    "\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipeline_fit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "lr = LogisticRegression(maxIter=50, regParam=0.3, elasticNetParam=0)\n",
    "# 为交叉验证创建参数\n",
    "# ParamGridBuilder：用于基于网格搜索的模型选择的参数网格的生成器\n",
    "# addGrid：将网格中给定参数设置为固定值\n",
    "# parameter：正则化参数\n",
    "# maxIter：迭代次数\n",
    "# numFeatures：特征值\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2])\n",
    "             .addGrid(lr.maxIter, [10, 50, 80])\n",
    "#              .addGrid(idf.numFeatures, [10, 100, 1000])\n",
    "             .build())\n",
    "\n",
    "# 创建五折交叉验证\n",
    "# estimator：要交叉验证的估计器\n",
    "# estimatorParamMaps：网格搜索的最优参数\n",
    "# evaluator：评估器\n",
    "# numFolds：交叉次数\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction')\n",
    "cv = CrossValidator(estimator=lr,\\\n",
    "                   estimatorParamMaps=paramGrid,\\\n",
    "                   evaluator=evaluator2,\\\n",
    "                   numFolds=10)\n",
    "cv_model = cv.fit(trainingData)\n",
    "predictions = cv_model.transform(testData)\n",
    " \n",
    "# 模型评估\n",
    "\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictions)))\n",
    "end_time = time.time()\n",
    "print(\"Running time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
